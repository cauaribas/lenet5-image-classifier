{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOU6jNdA7A4t"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfJkd6i-7Asy"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python seaborn\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# print(\"TensorFlow version:\", tf.__version__)\n",
        "# print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYu-fSWsjs-6"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpOniE1rjuE7"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (32, 32)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "VALIDATION_SPLIT = 0.2\n",
        "DATA_PATH = \"src\\dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GQOBD4hqKx-"
      },
      "source": [
        "## Testing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqF7kyVqqMYJ"
      },
      "outputs": [],
      "source": [
        "def explore_dataset(data_path):\n",
        "    for root, dirs, files in os.walk(data_path):\n",
        "        level = root.replace(data_path, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files[:5]:\n",
        "            print(f\"{subindent}{file}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"{subindent}... e mais {len(files) - 5} arquivos\")\n",
        "\n",
        "explore_dataset(DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgj1BF8TJSr5"
      },
      "source": [
        "## Preprocess Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_tph0_1JU1v"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path, target_size=IMAGE_SIZE):\n",
        "    # Carregar imagem\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "    # BGR para RGB\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Remoção de ruído usando filtro Gaussiano\n",
        "    image_denoised = cv2.GaussianBlur(image_rgb, (5, 5), 0)\n",
        "\n",
        "    # Redimensionar para tamanho padrão\n",
        "    image_resized = cv2.resize(image_denoised, target_size)\n",
        "\n",
        "    # Normalização (0-1)\n",
        "    image_normalized = image_resized / 255.0\n",
        "\n",
        "    return image_normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx97qc-dJjEp"
      },
      "source": [
        "## Load Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydbgIXacJlzo"
      },
      "outputs": [],
      "source": [
        "def load_sample_images(data_path, n_samples=5):\n",
        "    positive_path = os.path.join(data_path, \"positives\")\n",
        "    negative_path = os.path.join(data_path, \"negatives\")\n",
        "\n",
        "    print(f\"Positive path: {positive_path}\")\n",
        "    print(f\"Negative path: {negative_path}\")\n",
        "\n",
        "    # Carregar amostras\n",
        "    fig, axes = plt.subplots(2, n_samples, figsize=(15, 6))\n",
        "\n",
        "    # Imagens com rachadura\n",
        "    if os.path.exists(positive_path):\n",
        "        positive_files = [f for f in os.listdir(positive_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        for i in range(min(n_samples, len(positive_files))):\n",
        "            img_path = os.path.join(positive_path, positive_files[i])\n",
        "            img = cv2.imread(img_path)\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            axes[0, i].imshow(img_rgb)\n",
        "            axes[0, i].set_title(f\"COM RACHADURA\\n{positive_files[i]}\")\n",
        "            axes[0, i].axis(\"off\")\n",
        "\n",
        "    # Imagens sem rachadura\n",
        "    if os.path.exists(negative_path):\n",
        "        negative_files = [f for f in os.listdir(negative_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        for i in range(min(n_samples, len(negative_files))):\n",
        "            img_path = os.path.join(negative_path, negative_files[i])\n",
        "            img = cv2.imread(img_path)\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            axes[1, i].imshow(img_rgb)\n",
        "            axes[1, i].set_title(f\"SEM RACHADURA\\n{negative_files[i]}\")\n",
        "            axes[1, i].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3AFGaAhJsq3"
      },
      "source": [
        "## CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IULal4PjJty-"
      },
      "outputs": [],
      "source": [
        "def create_cnn_model(input_shape):\n",
        "    model = models.Sequential([\n",
        "        Conv2D(filters=6, kernel_size=(5, 5), padding='valid', activation='tanh', input_shape=input_shape),\n",
        "        AveragePooling2D(pool_size=(2, 2), strides=2, padding='valid'),\n",
        "\n",
        "        Conv2D(filters=16, kernel_size=(5, 5),  padding='valid', activation='tanh'),\n",
        "        AveragePooling2D(pool_size=(2, 2), strides=2,  padding='valid'),\n",
        "\n",
        "        Flatten(),\n",
        "\n",
        "        Dense(units=120, activation='tanh'),\n",
        "        Dense(units=84, activation='tanh'),\n",
        "        Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', 'precision', 'recall']\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onk_P3oCJ3Ua"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrgU2PyWJ4qu"
      },
      "outputs": [],
      "source": [
        "def load_dataset(data_path, sample_size=None):\n",
        "    # Procurar diretórios positivos e negativos (case-insensitive, plural/singular)\n",
        "    positive_path = os.path.join(data_path, \"Positive\")\n",
        "    negative_path = os.path.join(data_path, \"Negative\")\n",
        "\n",
        "    # Verificar estrutura alternativa se necessário\n",
        "    if not os.path.exists(positive_path):\n",
        "        for root, dirs, files in os.walk(data_path):\n",
        "            for d in dirs:\n",
        "                if 'positive' in d.lower() or 'crack' in d.lower():\n",
        "                    positive_path = os.path.join(root, d)\n",
        "                    break\n",
        "\n",
        "    if not os.path.exists(negative_path):\n",
        "        for root, dirs, files in os.walk(data_path):\n",
        "            for d in dirs:\n",
        "                if 'negative' in d.lower() or 'no' in d.lower():\n",
        "                    negative_path = os.path.join(root, d)\n",
        "                    break\n",
        "\n",
        "    print(f\"Positive path found: {positive_path} (exists: {os.path.exists(positive_path)})\")\n",
        "    print(f\"Negative path found: {negative_path} (exists: {os.path.exists(negative_path)})\")\n",
        "\n",
        "    if not os.path.exists(positive_path):\n",
        "        raise ValueError(f\"Diretório de imagens positivas não encontrado: {positive_path}\")\n",
        "    if not os.path.exists(negative_path):\n",
        "        raise ValueError(f\"Diretório de imagens negativas não encontrado: {negative_path}\")\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Carregar imagens positivas (com rachaduras)\n",
        "    if os.path.exists(positive_path):\n",
        "        positive_files = [f for f in os.listdir(positive_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        if sample_size:\n",
        "            positive_files = positive_files[:sample_size//2]\n",
        "\n",
        "        print(f\"Carregando {len(positive_files)} imagens COM rachadura...\")\n",
        "        for i, filename in enumerate(positive_files):\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"Processadas {i} imagens positivas...\")\n",
        "\n",
        "            img_path = os.path.join(positive_path, filename)\n",
        "            try:\n",
        "                img = preprocess_image(img_path)\n",
        "                images.append(img)\n",
        "                labels.append(1)  # Com rachadura\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao processar {filename}: {e}\")\n",
        "\n",
        "    # Carregar imagens negativas (sem rachaduras)\n",
        "    if os.path.exists(negative_path):\n",
        "        negative_files = [f for f in os.listdir(negative_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        if sample_size:\n",
        "            negative_files = negative_files[:sample_size//2]\n",
        "\n",
        "        print(f\"Carregando {len(negative_files)} imagens SEM rachadura...\")\n",
        "        for i, filename in enumerate(negative_files):\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"Processadas {i} imagens negativas...\")\n",
        "\n",
        "            img_path = os.path.join(negative_path, filename)\n",
        "            try:\n",
        "                img = preprocess_image(img_path)\n",
        "                images.append(img)\n",
        "                labels.append(0)  # Sem rachadura\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao processar {filename}: {e}\")\n",
        "\n",
        "    if not images:\n",
        "        raise ValueError(\"Nenhuma imagem foi carregada. Verifique o caminho do dataset.\")\n",
        "\n",
        "    X = np.array(images)\n",
        "    y = np.array(labels)\n",
        "\n",
        "    print(f\"\\nDataset carregado com sucesso!\")\n",
        "    print(f\"Total de imagens: {len(X)}\")\n",
        "    print(f\"Imagens com rachadura: {np.sum(y)}\")\n",
        "    print(f\"Imagens sem rachadura: {len(y) - np.sum(y)}\")\n",
        "    print(f\"Shape das imagens: {X.shape}\")\n",
        "    print(f\"Shape dos labels: {y.shape}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# X, y = load_dataset(DATA_PATH, sample_size=10000)\n",
        "X, y = load_dataset(DATA_PATH)  # Para dataset completo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQxVwItUJ8-c"
      },
      "source": [
        "# Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nLbOjMPJ_Oi"
      },
      "outputs": [],
      "source": [
        "def prepare_data(X, y, test_size=0.2):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Conjunto de treino: {len(X_train)} imagens\")\n",
        "    print(f\"Conjunto de teste: {len(X_test)} imagens\")\n",
        "    print(f\"Distribuição treino - Com rachadura: {np.sum(y_train)}, Sem rachadura: {len(y_train) - np.sum(y_train)}\")\n",
        "    print(f\"Distribuição teste - Com rachadura: {np.sum(y_test)}, Sem rachadura: {len(y_test) - np.sum(y_test)}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = prepare_data(X, y)\n",
        "model = create_cnn_model(X_train.shape[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY68YePEKUYL"
      },
      "source": [
        "## Data Augumentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D72JPwxpKVmi"
      },
      "outputs": [],
      "source": [
        "def create_data_generators(X_train, y_train, batch_size=BATCH_SIZE):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest',\n",
        "        validation_split=VALIDATION_SPLIT\n",
        "    )\n",
        "\n",
        "    train_generator = datagen.flow(\n",
        "        X_train, y_train,\n",
        "        batch_size=batch_size,\n",
        "        subset='training',\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    validation_generator = datagen.flow(\n",
        "        X_train, y_train,\n",
        "        batch_size=batch_size,\n",
        "        subset='validation',\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    return train_generator, validation_generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4F4TsSaKbW2"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUWiQ_iqKcXB"
      },
      "outputs": [],
      "source": [
        "def train_model(model, X_train, y_train, use_augmentation=True):\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    if use_augmentation:\n",
        "        # Treinamento com data augmentation\n",
        "        train_gen, val_gen = create_data_generators(X_train, y_train)\n",
        "\n",
        "        history = model.fit(\n",
        "            train_gen,\n",
        "            epochs=EPOCHS,\n",
        "            validation_data=val_gen,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "    else:\n",
        "        # Treinamento sem data augmentation\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=EPOCHS,\n",
        "            validation_split=VALIDATION_SPLIT,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    return history\n",
        "\n",
        "history = train_model(model, X_train, y_train, use_augmentation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvpkXV15Kxhn"
      },
      "source": [
        "## Model Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbNQ3XSaKy-S"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test, history):\n",
        "    # Predições\n",
        "    y_pred_proba = model.predict(X_test)\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Métricas\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\n=== RESULTADOS DA AVALIAÇÃO ===\")\n",
        "    print(f\"Acurácia no conjunto de teste: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "    # Verificar se atende ao requisito de 80%\n",
        "    if accuracy >= 0.8:\n",
        "        print(\"✅ APROVADO: Acurácia >= 80%\")\n",
        "    else:\n",
        "        print(\"❌ REPROVADO: Acurácia < 80%\")\n",
        "\n",
        "    # Relatório detalhado\n",
        "    print(\"\\n=== RELATÓRIO DE CLASSIFICAÇÃO ===\")\n",
        "    print(classification_report(y_test, y_pred,\n",
        "                              target_names=['Sem Rachadura', 'Com Rachadura']))\n",
        "\n",
        "    # Matriz de confusão\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Sem Rachadura', 'Com Rachadura'],\n",
        "                yticklabels=['Sem Rachadura', 'Com Rachadura'])\n",
        "    plt.title('Matriz de Confusão')\n",
        "    plt.ylabel('Valor Real')\n",
        "    plt.xlabel('Predição')\n",
        "    plt.show()\n",
        "\n",
        "    # Plotar histórico de treinamento\n",
        "    plot_training_history(history)\n",
        "\n",
        "    return accuracy, y_pred_proba\n",
        "\n",
        "def plot_training_history(history):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "    # Acurácia\n",
        "    axes[0, 0].plot(history.history['accuracy'], label='Treino')\n",
        "    axes[0, 0].plot(history.history['val_accuracy'], label='Validação')\n",
        "    axes[0, 0].set_title('Acurácia')\n",
        "    axes[0, 0].set_xlabel('Época')\n",
        "    axes[0, 0].set_ylabel('Acurácia')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    # Loss\n",
        "    axes[0, 1].plot(history.history['loss'], label='Treino')\n",
        "    axes[0, 1].plot(history.history['val_loss'], label='Validação')\n",
        "    axes[0, 1].set_title('Loss')\n",
        "    axes[0, 1].set_xlabel('Época')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    # Precisão\n",
        "    if 'precision' in history.history:\n",
        "        axes[1, 0].plot(history.history['precision'], label='Treino')\n",
        "        axes[1, 0].plot(history.history['val_precision'], label='Validação')\n",
        "        axes[1, 0].set_title('Precisão')\n",
        "        axes[1, 0].set_xlabel('Época')\n",
        "        axes[1, 0].set_ylabel('Precisão')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True)\n",
        "\n",
        "    # Recall\n",
        "    if 'recall' in history.history:\n",
        "        axes[1, 1].plot(history.history['recall'], label='Treino')\n",
        "        axes[1, 1].plot(history.history['val_recall'], label='Validação')\n",
        "        axes[1, 1].set_title('Recall')\n",
        "        axes[1, 1].set_xlabel('Época')\n",
        "        axes[1, 1].set_ylabel('Recall')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "accuracy, predictions = evaluate_model(model, X_test, y_test, history)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}